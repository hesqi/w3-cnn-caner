{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11848,"databundleVersionId":862157,"sourceType":"competition"}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport os,sys,csv\nimport cv2 as cv\nimport matplotlib.pyplot as plt\n\ntf.enable_eager_execution()\ntf.VERSION\nAUTOTUNE = tf.data.experimental.AUTOTUNE \n\nIMAGE_COUNT_TRAIN = 0\nIMAGE_COUNT_TEST = 0\nBATCH_SIZE = 36\n\ndef prepare_dataset_trian_test():\n    def prepare_filenames_labels_all():\n        CSV_PATH = 'D:/ai_data/histopathologic-cancer-detection/train_labels.csv'\n        TRAIN_IMAGE_FOLDER = 'D:/ai_data/histopathologic-cancer-detection/train'\n\n    def read_csv():\n        with open(CSV_PATH) as f:\n            reader = csv.reader(f)\n            return list(reader)\n\n    csv_read = read_csv()\n    filenames = []\n    labels = []\n    \n    for item in csv_read[1:]:\n        filenames.append(TRAIN_IMAGE_FOLDER + '/' + item[0]+'.tif')\n        labels.append(int(item[1]))\n    # return filenames[:1000],labels[:1000]\n    return filenames,labels\n\n  def prepare_filenames_labels_train_validate(filenames_all,labels_all,validate_ratio):\n    global IMAGE_COUNT_TRAIN\n    global IMAGE_COUNT_TEST\n    file_quant = len(filenames_all)\n    file_quant_train = int(float(file_quant)*(1.0-validate_ratio))\n    # file_quant_test = file_quant - file_quant_train\n    train_filenames = filenames_all[0:file_quant_train]\n    train_labels = labels_all[0:file_quant_train]\n    test_filenames = filenames_all[file_quant_train:]\n    test_labels = labels_all[file_quant_train:]\n    IMAGE_COUNT_TRAIN = len(train_filenames)\n    IMAGE_COUNT_TEST = len(test_filenames)\n    return train_filenames,train_labels,test_filenames,test_labels\n\n  filenames_all,labels_all = prepare_filenames_labels_all()\n  train_filenames,train_labels,test_filenames,test_labels = prepare_filenames_labels_train_validate(filenames_all,labels_all,0.2)\n\n  def image_read_cv2(filename,label):\n    image_decoded = cv.imread(filename.numpy().decode(), 1)\n    return image_decoded, label\n\n  def image_resize(image_decoded,label):\n    image_decoded.set_shape([None, None, None])\n    image_resized = tf.image.resize_images(image_decoded, [299, 299])\n    return (image_resized / 255.0 - 0.5)*2 , tf.one_hot(label,2,on_value=1.0,off_value=0.0,axis=-1)\n\n  def prepare_train_ds(filenames,labels):\n    global BATCH_SIZE\n    paths_ds = tf.data.Dataset.from_tensor_slices(filenames)\n    labels_ds = tf.data.Dataset.from_tensor_slices(labels)\n    paths_labels_ds = tf.data.Dataset.zip((paths_ds,labels_ds))\n    images_labels_ds = paths_labels_ds.shuffle(buffer_size=300000)\n    images_labels_ds = images_labels_ds.map(lambda filename,label : tf.py_function( func=image_read_cv2,\n                                                                                    inp=[filename,label],\n                                                                                    Tout=[tf.uint8,tf.int32]),\n                                                                                    num_parallel_calls=AUTOTUNE)\n    images_labels_ds = images_labels_ds.map(image_resize,num_parallel_calls=AUTOTUNE)\n    images_labels_ds = images_labels_ds.repeat()\n    images_labels_ds = images_labels_ds.batch(BATCH_SIZE)\n    images_labels_ds = images_labels_ds.prefetch(buffer_size = 200)\n    \n    # plt.figure(figsize=(8,8))\n    # for n,(image,label) in enumerate(images_labels_ds.take(10)):\n    #   plt.subplot(2,5,n+1)\n    #   plt.imshow(image)\n    #   plt.grid(False)\n    #   plt.xticks([])\n    #   plt.yticks([])\n    #   plt.xlabel('xxxxxxxxx label')\n    # plt.show()\n    \n    return images_labels_ds\n\n  def prepare_test_ds(filenames,labels):\n    global BATCH_SIZE\n    paths_ds = tf.data.Dataset.from_tensor_slices(filenames)\n    labels_ds = tf.data.Dataset.from_tensor_slices(labels)\n    images_labels_ds = tf.data.Dataset.zip((paths_ds,labels_ds))\n    # images_labels_ds = images_labels_ds.shuffle(buffer_size=300000)\n    images_labels_ds = images_labels_ds.map(lambda filename,label : tf.py_function( func=image_read_cv2,\n                                                                                    inp=[filename,label],\n                                                                                    Tout=[tf.uint8,tf.int32]),\n                                                                                    num_parallel_calls=AUTOTUNE)\n    images_labels_ds = images_labels_ds.map(image_resize,num_parallel_calls=AUTOTUNE)\n    images_labels_ds = images_labels_ds.repeat()\n    images_labels_ds = images_labels_ds.batch(BATCH_SIZE)\n    images_labels_ds = images_labels_ds.prefetch(buffer_size = 200)\n    \n    return images_labels_ds\n\n  train_image_label_ds = prepare_train_ds(train_filenames,train_labels)\n  test_image_label_ds = prepare_test_ds(test_filenames,test_labels)\n\n  return train_image_label_ds,test_image_label_ds\n\ntrain_image_label_ds,test_image_label_ds = prepare_dataset_trian_test()\n\nkeras_ds = train_image_label_ds\nkeras_validate_ds = test_image_label_ds\n\nInceptionV3 = tf.keras.applications.InceptionV3(include_top=False, \n                                                      weights='imagenet',\n                                                      input_tensor=None, \n                                                      input_shape=(299, 299, 3))\n\n\n\ninception_v3 = InceptionV3.get_layer(index = -1).output\noutput = tf.keras.layers.AveragePooling2D((8, 8), strides=(8, 8), name='avg_pool')(inception_v3)\noutput = tf.keras.layers.Flatten(name='flatten')(output)\noutput = tf.keras.layers.Dense(2, activation='softmax', name='predictions')(output)\n\nmodel = tf.keras.models.Model(InceptionV3.input, output)\n\nmodel.trainable = True\n# for layer in model.layers[:-3]:\n#   layer.trainable = False\n\nmodel.summary()\n\n# for x in model.non_trainable_weights:\n#   print(x.name)\n\noptimizer = tf.keras.optimizers.SGD(lr = 0.0001, momentum = 0.9, decay = 0.0, nesterov = True)\nmodel.compile(loss='binary_crossentropy', \n              optimizer = optimizer, \n              metrics = ['accuracy'])\n\n\nsteps_per_epoch = tf.ceil(IMAGE_COUNT_TRAIN/BATCH_SIZE)\nvalidation_steps = tf.ceil(IMAGE_COUNT_TEST/BATCH_SIZE)\n\ncheckpoint_path = 'checkpoint4/cp.ckpt'\ncheckpoint_dir = os.path.dirname(checkpoint_path)\n\ncp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n                                                 save_weights_only=True,\n                                                 save_best_only=True,\n                                                 monitor='val_acc',\n                                                 mode='max',\n                                                 verbose = 1)\n\ntb_callback = tf.keras.callbacks.TensorBoard(log_dir='./Graph4', update_freq='batch',histogram_freq=0, write_graph=True, write_images=True)\n\nmodel.load_weights(checkpoint_path)\n\nmodel.fit(keras_ds,\n          epochs=8,\n          steps_per_epoch=steps_per_epoch,\n          # validation_split = 0.2,\n          validation_data = keras_validate_ds,\n          validation_steps = validation_steps,\n          callbacks = [cp_callback,tb_callback]\n          )\n\nprint()\nprint()\nprint('>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>evaluate>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n\nmodel.load_weights(checkpoint_path)\n\nloss,acc = model.evaluate(keras_validate_ds,steps=validation_steps)\nprint(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))```\n\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-05T09:00:48.948015Z","iopub.execute_input":"2024-03-05T09:00:48.948491Z","iopub.status.idle":"2024-03-05T09:00:48.954126Z","shell.execute_reply.started":"2024-03-05T09:00:48.948458Z","shell.execute_reply":"2024-03-05T09:00:48.953248Z"},"trusted":true},"execution_count":6,"outputs":[]}]}